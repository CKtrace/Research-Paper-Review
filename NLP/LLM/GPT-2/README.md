# [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (OpenAI)

<br>

## Abstract

<br>

LM이 다양한 NLP task를 explicit supervision없이 수 백만 개의 webpages인 WebText dataset으로 학습 가능함을 입증했다.

또한 127,000+ 개의 training examples 없이 CoQA에서 55 F1을 달성했고, 이는 4개의 baseline 중 3개를 뛰어넘는 수치이다.

본 논문에서 제안하는 GPT-2는 1.5B parameter Transformer이고, 이는 zero-shot setting으로 Language Modeling Datasets의 8개 중 7개에서 SOTA를 달성했으며, 여전히 WebText에는 underfit 되어있다.

<br>

## Introduction

<br>

본 연구의 목표는 다양한 task를 소화할 수 있는 더욱 General system을 만들어내는 것이다.


TBU


