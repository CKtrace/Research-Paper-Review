{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7213b7f-c535-4f0a-a3b4-b737697992bc",
   "metadata": {},
   "source": [
    "## Example Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f49224c-4286-49ca-8dd5-3ab901517ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"\"\"In the case of CBOW, one word is eliminated, and the word is predicted from surrounding words.\n",
    "Therefore, it takes multiple input vectors as inputs to the model and creates one output vector.\n",
    "In contrast, Skip-Gram learns by removing all words except one word and predicting the surrounding words in the context through one word. \n",
    "So, it takes a vector as input and produces multiple output vectors.\n",
    "CBOW and Skip-Gram are different.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f2e21-972b-4183-a91c-fad20faeb21e",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaefd79-8143-4f03-9e87-88db61a703c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 | Loss = 302.3144836425781\n",
      "Epoch = 100 | Loss = 223.91552734375\n",
      "Epoch = 200 | Loss = 152.63931274414062\n",
      "Epoch = 300 | Loss = 94.65900421142578\n",
      "Epoch = 400 | Loss = 54.733489990234375\n",
      "Epoch = 500 | Loss = 31.783037185668945\n",
      "Epoch = 600 | Loss = 19.998750686645508\n",
      "Epoch = 700 | Loss = 13.860055923461914\n",
      "Epoch = 800 | Loss = 10.344636917114258\n",
      "Epoch = 900 | Loss = 8.128987312316895\n",
      "Prediction : Skip-Gram\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def CBOW_make_context_vector(context, word_to_idx):\n",
    "    idxs = [word_to_idx[word] for word in context]\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "def CBOW_make_data(sentence):\n",
    "    data = []\n",
    "    for i in range(2, len(example_sentence) - 2):\n",
    "        context = [example_sentence[i - 2], example_sentence[i - 1], example_sentence[i + 1], example_sentence[i + 2]]\n",
    "        target = example_sentence[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.layer1 = nn.Linear(embedding_dim, 64)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        \n",
    "        self.layer2 = nn.Linear(64, vocab_size)\n",
    "        self.activation2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = sum(self.embeddings(X)).view(1, -1)\n",
    "        X = self.activation1(self.layer1(X))\n",
    "        X = self.activation2(self.layer2(X))\n",
    "        return X\n",
    "        \n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "Epochs = 1000\n",
    "\n",
    "vocab = set(example_sentence)\n",
    "vocab_size = len(example_sentence)\n",
    "\n",
    "word_to_idx = {word : idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx : word for idx, word in enumerate(vocab)}\n",
    "\n",
    "data = CBOW_make_data(example_sentence)\n",
    "\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = CBOW_make_context_vector(context, word_to_idx)\n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_idx[target]]))\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch = {epoch} | Loss = {total_loss}')\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "test_data = ['CBOW', 'and', 'are', 'different.']\n",
    "test_vector = CBOW_make_context_vector(test_data, word_to_idx)\n",
    "result = model(test_vector)\n",
    "print(f\"Prediction : {idx_to_word[torch.argmax(result[0]).item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e938220-c9b8-4ff2-b412-415023d55622",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575a8c0a-0edc-4c44-8bcb-470e1223ddfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Loss : 389.0058898925781\n",
      "Epoch : 100 | Loss : 328.5797119140625\n",
      "Epoch : 200 | Loss : 232.27313232421875\n",
      "Epoch : 300 | Loss : 163.70765686035156\n",
      "Epoch : 400 | Loss : 141.14642333984375\n",
      "Epoch : 500 | Loss : 134.49583435058594\n",
      "Epoch : 600 | Loss : 132.18878173828125\n",
      "Epoch : 700 | Loss : 131.12271118164062\n",
      "Epoch : 800 | Loss : 130.5124969482422\n",
      "Epoch : 900 | Loss : 130.1202850341797\n",
      "Prediction : ['CBOW', 'and', 'learns', 'by']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "EPOCHS = 1000\n",
    "CONTEXT_SIZE = 4\n",
    "\n",
    "def Skip_gram_make_context_vector(context, word_to_idx):\n",
    "    idxs = word_to_idx[context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def Skip_gram_make_data(sentence):\n",
    "    data = []\n",
    "    for i in range(2, len(example_sentence) - 2):\n",
    "        # CBOW와 context, target 분리 작업 과정이 반대\n",
    "        context = example_sentence[i]\n",
    "        target = [example_sentence[i - 2], example_sentence[i - 1], example_sentence[i + 1], example_sentence[i + 2]]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "class SKIP_GRAM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SKIP_GRAM, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.layer1 = nn.Linear(embedding_dim, 64)\n",
    "        self.activation1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(64, vocab_size * context_size)\n",
    "        self.activation2 = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embeddings(X)\n",
    "        X = self.activation1(self.layer1(X))\n",
    "        X = self.activation2(self.layer2(X))\n",
    "        return X.view(self.context_size, vocab_size)\n",
    "\n",
    "vocab = set(example_sentence)\n",
    "vocab_size = len(example_sentence)\n",
    "\n",
    "word_to_idx = {word : idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx : word for idx, word in enumerate(vocab)}\n",
    "\n",
    "data = Skip_gram_make_data(example_sentence)\n",
    "\n",
    "model = SKIP_GRAM(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_vector = Skip_gram_make_context_vector(context, word_to_idx)\n",
    "        log_probs = model(context_vector)\n",
    "        total_loss += loss_function(log_probs, torch.LongTensor([word_to_idx[t] for t in target]))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch : {epoch} | Loss : {total_loss}\")\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "test_data = 'Skip-Gram'\n",
    "test_vector = Skip_gram_make_context_vector(test_data, word_to_idx)\n",
    "result = model(test_vector)\n",
    "print(f\"Prediction : {[idx_to_word[torch.argmax(r).item()] for r in result]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_fine",
   "language": "python",
   "name": "llm_fine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
