# NLP - Paper List

<br>

|Title|Journal/Conference|Year|Progress|
|---|:---:|---|:---:|
|[A Neural Probabilistic Language Model (NNLM)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/NNLM)|JLMR|2003|Clear!|
|[Efficient Estimation of Word Representations in Vectore Space (Word2Vec)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/Word2Vec)|ICLR|2013|Clear!|
|[Enriching Word Vectors with Subword Information (FastText)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/FastText)|ACL|2017|Clear!|
|[Convolutional Neural Networks for Sentence Classification (TextCNN)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/TextCNN)|EMNLP|2014|Clear!|
|[Finding Structure in Time (TextRNN)](https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1)|CogSCI|1990|Clear!|
|[LONG SHORT-TERM MEMORY (TextLSTM)](https://www.bioinf.jku.at/publications/older/2604.pdf)|Neural Computation|1997|Clear!|
|[Sequence to Sequence with Neural Networks (Seq2Seq)](https://arxiv.org/pdf/1406.1078)|NIPS|2014|Clear!|
|[Attention is all you need (Transformer)](https://arxiv.org/pdf/1706.03762)|NIPS|2017|Clear!|
<br>

## LLM - Paper List

<br>

|Title|Journal/Conference|Year|Progress|
|---|:---:|---|:---:|
|[BERT: Pre-traning of Deep Bidrectional Tranformers for Language Understanding](https://arxiv.org/pdf/1810.04805)|ACL|2019|Clear!|
|[Improving Language Understanding by Generative Pre-Training (GPT-1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)|OpenAI|2018|Clear!|
|[Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|OpenAI|2019|Clear!|
|[TRANSFORMER-XL: LANGUAGE MODELING WITH LONGER-TERM DEPENDENCY](https://openreview.net/pdf?id=HJePno0cYm)|ICLR|2019|Clear!|
|[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237)|NIPS|2019|TBU|
|[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer(T3)](https://dl.acm.org/doi/pdf/10.5555/3455716.3455856)|JMLR|2020|TBU|
|[Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/pdf/2005.14165)|NIPS|2020|TBU|
|[LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239)|Google I/O|2021|TBU|
|[Pathway Language Model: A Novel Neural Architecture for Natural Language Processing](https://arxiv.org/pdf/2204.02311)|arXiv|2022|TBU|
|[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971)|arXiv|2023|TBU|

```
추후 정리....
GPT_4: A Large Multimodal Model for Generating Text, Translating Languages, Writing defferent Kinds of Creative Content, and Answering Your Questions in a Informative Way(2023)
PaLM-E: An Embodied Multimodal Language Model (2023)
LLaMa: Open and Efficient Foundation Language Models (2023)
Claude 2: A Large Language Model for the Next Generation of (2023)
HyperCLOVAX: A Korean-specific Large Language Model for the Next Generation (2023)
```

## Hallucination - Paper List

<br>

|Title|Journal/Conference|Year|Progress|
|---|:---:|---|:---:|
|[SELFCHECKGPT : Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/pdf/2303.08896)|ACL|2023|Clear!|
|[A Survey of Hallucination in "Large" Foundation Models](https://arxiv.org/pdf/2309.05922)|arXiv|2023|Clear!|
|[Evaluating Object Hallucination in Large Vision-Language Models](https://arxiv.org/pdf/2305.10355)|EMNLP|2023|Clear!|
