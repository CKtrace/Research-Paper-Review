# NLP - Paper List

<br>

|Title|Journal/Conference|Year|Progress|
|---|:---:|---|:---:|
|[A Neural Probabilistic Language Model (NNLM)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/NNLM)|JLMR|2003|Clear!|
|[Efficient Estimation of Word Representations in Vectore Space (Word2Vec)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/Word2Vec)|ICLR|2013|Clear!|
|[Enriching Word Vectors with Subword Information (FastText)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/FastText)|ACL|2017|Clear!|
|[Convolutional Neural Networks for Sentence Classification (TextCNN)](https://github.com/CKtrace/Research-Paper-Review/tree/main/NLP/TextCNN)|EMNLP|2014|Clear!|
|[Finding Structure in Time (TextRNN)](https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1)|CogSCI|1990|Clear!|
|[LONG SHORT-TERM MEMORY (TextLSTM)](https://www.bioinf.jku.at/publications/older/2604.pdf)|Neural Computation|1997|TBU|
|[Sequence to Sequence with Neural Networks (Seq2Seq)](https://arxiv.org/pdf/1406.1078)|NIPS|2014|TBU|
|[NEURAL MACHINE TRANSLATIONBY JOINTLY LEARNING TO ALIGN AND TRANSLATE (Seq2Seq with Attention)](https://arxiv.org/pdf/1409.0473)|ICLR|2015|TBU|

<br>

## LLM - Paper List

<br>

|Title|Journal/Conference|Year|Progress|
|---|:---:|---|:---:|
|[Attention is all you need (Transformer)](https://arxiv.org/pdf/1706.03762)|NIPS|2017|TBU|
|[BERT: Pre-traning of Deep Bidrectional Tranformers for Language Understanding](https://arxiv.org/pdf/1810.04805)|ACL|2019|TBU|
|[Improving Language Understanding by Generative Pre-Training (GPT-1)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)|OpenAI|2018|TBU|
|[Language Models are Unsupervised Multitask Learners (GPT-2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|OpenAI|2019|TBU|
|[TRANSFORMER-XL: LANGUAGE MODELING WITH LONGER-TERM DEPENDENCY](https://openreview.net/pdf?id=HJePno0cYm)|ICLR|2019|TBU|

```
추후 정리....

XLNet: Generalized Autoregressive Pretraining for Language Understanding (2019)
T3: Exploring the Limits of Transfer Learning with Text-to-Text Transfomer (2020)
GPT-3: Language Models are Few-Shot Learners(2020)
LaMDA: Language Models for Dialogue Applications (2021)
Pathway Language Model: A Novel Neural Architecture for Natural Language Processing (2022)
LLaMa: Open and Efficient Foundation Language Models(2023)
GPT_4: A Large Multimodal Model for Generating Text, Translating Languages, Writing defferent Kinds of Creative Content, and Answering Your Questions in a Informative Way(2023)
PaLM-E: An Embodied Multimodal Language Model (2023)
LLaMa: Open and Efficient Foundation Language Models (2023)
Claude 2: A Large Language Model for the Next Generation of (2023)
HyperCLOVAX: A Korean-specific Large Language Model for the Next Generation (2023)
```
